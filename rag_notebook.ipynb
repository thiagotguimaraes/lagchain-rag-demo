{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d65ae350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from openai_client import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "472a7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables. Assumes that project contains .env file with API keys\n",
    "load_dotenv()\n",
    "\n",
    "#---- Set OpenAI API key \n",
    "# Change environment variable name from \"OPENAI_API_KEY\" to the name given in \n",
    "# your .env file.\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "CHROMA_COLLECTION = \"alice\"\n",
    "DATA_PATH = \"data/\"\n",
    "EMBEDDINGS_MODEL=\"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeed9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out the database first.\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# Setup Chroma client with local persistence\n",
    "chroma_client = chromadb.Client(\n",
    "    chromadb.config.Settings(\n",
    "        persist_directory=CHROMA_PATH,  # Folder to store Chroma DB\n",
    "    )\n",
    ")\n",
    "\n",
    "# Use SentenceTransformer for embeddings\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDINGS_MODEL\n",
    ")\n",
    "\n",
    "# Load or create collection\n",
    "collection_name = CHROMA_COLLECTION\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=collection_name,\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "# Clear db system cache if any\n",
    "chroma_client.clear_system_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b055128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(data_path: str=DATA_PATH)->List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Directory path.\n",
    "\n",
    "    Returns:\n",
    "        documents: A list of Document objects.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: List[Document]):\n",
    "    \"\"\"\n",
    "    Split documents in chunks.\n",
    "    \n",
    "    Args:\n",
    "        documents: A list of Documents objects.\n",
    "\n",
    "    Returns:\n",
    "        chunks: A list of Document objects divided in chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def add_documents(docs: Optional[Document], ids=None , metadatas=None):\n",
    "    \"\"\"\n",
    "    Add documents to Chroma colection.\n",
    "    \n",
    "    Args:\n",
    "        docs: Optional list of Documents objects.\n",
    "        ids: Optional list of Documents objects ids.\n",
    "        metadata: Optional list of Documents objects metadatas.\n",
    "\n",
    "    Returns:\n",
    "        void.\n",
    "    \"\"\"\n",
    "    ids = ids or [f\"doc_{i}\" for i in range(len(docs))]\n",
    "    metadatas = metadatas or [{} for _ in docs]\n",
    "    collection.add(documents=docs, ids=ids, metadatas=metadatas)\n",
    "    \n",
    "\n",
    "def retrieve_documents(query: str, k=3)->Document:\n",
    "    \"\"\"\n",
    "    Retrieve documents from Chroma colection.\n",
    "    \n",
    "    Args:\n",
    "        query_texts: The document texts to get the closes neighbors of. Optional..\n",
    "        n_results: The number of neighbors to return for each query_embedding or query_texts. Optional..\n",
    "\n",
    "    Returns:\n",
    "        document: The first Document object of the query result.\n",
    "    \"\"\"\n",
    "    results = collection.query(query_texts=[query], n_results=k)\n",
    "    print('\\nResults:\\n', results)\n",
    "    document = results['documents'][0]\n",
    "    return document  # List of docs\n",
    "\n",
    "\n",
    "def gpt4o_text(prompt: str):\n",
    "    \"\"\"\n",
    "    Call OpenAi client model.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Prompt to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        answer: Model answer.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[\n",
    "            { \"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}] }\n",
    "        ],\n",
    "        model=\"gpt-4o\", # https://github.com/marketplace/models\n",
    "        temperature=1,\n",
    "        max_tokens=4096,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def run_rag_pipeline(query):\n",
    "    \"\"\"\n",
    "    Run RAG Pipeline Model.\n",
    "    \n",
    "    Args:\n",
    "        query: Query to retrive context from database by similarity.\n",
    "\n",
    "    Returns:\n",
    "        answer: RAG Pipeline Model answer.\n",
    "    \"\"\"\n",
    "    context_docs = retrieve_documents(query)\n",
    "    context = \"\\n\".join(context_docs)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    print('\\nPrompt:\\n', prompt)\n",
    "    return gpt4o_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46caa034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 818 chunks.\n",
      "\n",
      "Chunk sample:\n",
      " page_content='The Project Gutenberg eBook of Alice's Adventures in Wonderland' metadata={'source': 'data/alice_in_worderland.md', 'start_index': 0}\n",
      "\n",
      "Content sample:\n",
      " The Project Gutenberg eBook of Alice's Adventures in Wonderland\n",
      "\n",
      "Metadata sample:\n",
      " {'source': 'data/alice_in_worderland.md', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "chunks = split_text(documents)\n",
    "print('\\nChunk sample:\\n', chunks[0])\n",
    "\n",
    "contents = [chunk.page_content for chunk in chunks]\n",
    "print('\\nContent sample:\\n', contents[0])\n",
    "\n",
    "metadatas = [chunk.metadata for chunk in chunks]\n",
    "print('\\nMetadata sample:\\n', metadatas[0])\n",
    "\n",
    "add_documents(contents, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f5374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query:\n",
      " How does the Project Gutenberg is financed?\n",
      "\n",
      "Results:\n",
      " {'ids': [['doc_796', 'doc_733', 'doc_797']], 'embeddings': None, 'documents': [['Section 2. Information about the Mission of Project Gutenberg™', 'To protect the Project Gutenberg™ mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase “Project Gutenberg”), you agree to comply with all the terms of the Full Project Gutenberg™ License available', 'Project Gutenberg™ is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers. It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[{'source': 'data/alice_in_worderland.md', 'start_index': 158216}, {'source': 'data/alice_in_worderland.md', 'start_index': 145770}, {'source': 'data/alice_in_worderland.md', 'start_index': 158280}]], 'distances': [[0.5325243473052979, 0.591663122177124, 0.5952984690666199]]}\n",
      "\n",
      "Prompt:\n",
      " Use the following context to answer the question.\n",
      "\n",
      "Context:\n",
      "Section 2. Information about the Mission of Project Gutenberg™\n",
      "To protect the Project Gutenberg™ mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase “Project Gutenberg”), you agree to comply with all the terms of the Full Project Gutenberg™ License available\n",
      "Project Gutenberg™ is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers. It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.\n",
      "\n",
      "Question:\n",
      "How does the Project Gutenberg is financed?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the Project Gutenberg is financed?\"\n",
    "print('\\nQuery:\\n', query)\n",
    "\n",
    "answer = run_rag_pipeline(query)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
